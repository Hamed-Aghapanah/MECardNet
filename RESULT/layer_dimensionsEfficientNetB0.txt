layer 1 => input_tensor                  [(None, 256, 256, 3)]                     param = 1.0 
layer 2 => rescaling_10                  (None, 256, 256, 3)                  param = 196608 
layer 3 => normalization_7               (None, 256, 256, 3)                  param = 196608 
layer 4 => stem_conv_pad                 (None, 257, 257, 3)                  param = 198147 
layer 5 => stem_conv                     (None, 128, 128, 32)                  param = 524288 
layer 6 => stem_bn                       (None, 128, 128, 32)                  param = 524288 
layer 7 => stem_activation               (None, 128, 128, 32)                  param = 524288 
layer 8 => block1a_dwconv                (None, 128, 128, 32)                  param = 524288 
layer 9 => block1a_bn                    (None, 128, 128, 32)                  param = 524288 
layer 10 => block1a_activation            (None, 128, 128, 32)                  param = 524288 
layer 11 => block1a_se_squeeze            (None, 32)                    param = 32 
layer 12 => block1a_se_reshape            (None, 1, 1, 32)                  param = 32 
layer 13 => block1a_se_reduce             (None, 1, 1, 8)                  param = 8 
layer 14 => block1a_se_expand             (None, 1, 1, 32)                  param = 32 
layer 15 => block1a_se_excite             (None, 128, 128, 32)                  param = 524288 
layer 16 => block1a_project_conv          (None, 128, 128, 16)                  param = 262144 
layer 17 => block1a_project_bn            (None, 128, 128, 16)                  param = 262144 
layer 18 => block2a_expand_conv           (None, 128, 128, 96)                  param = 1572864 
layer 19 => block2a_expand_bn             (None, 128, 128, 96)                  param = 1572864 
layer 20 => block2a_expand_activation     (None, 128, 128, 96)                  param = 1572864 
layer 21 => block2a_dwconv_pad            (None, 129, 129, 96)                  param = 1597536 
layer 22 => block2a_dwconv                (None, 64, 64, 96)                  param = 393216 
layer 23 => block2a_bn                    (None, 64, 64, 96)                  param = 393216 
layer 24 => block2a_activation            (None, 64, 64, 96)                  param = 393216 
layer 25 => block2a_se_squeeze            (None, 96)                    param = 96 
layer 26 => block2a_se_reshape            (None, 1, 1, 96)                  param = 96 
layer 27 => block2a_se_reduce             (None, 1, 1, 4)                  param = 4 
layer 28 => block2a_se_expand             (None, 1, 1, 96)                  param = 96 
layer 29 => block2a_se_excite             (None, 64, 64, 96)                  param = 393216 
layer 30 => block2a_project_conv          (None, 64, 64, 24)                  param = 98304 
layer 31 => block2a_project_bn            (None, 64, 64, 24)                  param = 98304 
layer 32 => block2b_expand_conv           (None, 64, 64, 144)                  param = 589824 
layer 33 => block2b_expand_bn             (None, 64, 64, 144)                  param = 589824 
layer 34 => block2b_expand_activation     (None, 64, 64, 144)                  param = 589824 
layer 35 => block2b_dwconv                (None, 64, 64, 144)                  param = 589824 
layer 36 => block2b_bn                    (None, 64, 64, 144)                  param = 589824 
layer 37 => block2b_activation            (None, 64, 64, 144)                  param = 589824 
layer 38 => block2b_se_squeeze            (None, 144)                    param = 144 
layer 39 => block2b_se_reshape            (None, 1, 1, 144)                  param = 144 
layer 40 => block2b_se_reduce             (None, 1, 1, 6)                  param = 6 
layer 41 => block2b_se_expand             (None, 1, 1, 144)                  param = 144 
layer 42 => block2b_se_excite             (None, 64, 64, 144)                  param = 589824 
layer 43 => block2b_project_conv          (None, 64, 64, 24)                  param = 98304 
layer 44 => block2b_project_bn            (None, 64, 64, 24)                  param = 98304 
layer 45 => block2b_drop                  (None, 64, 64, 24)                  param = 98304 
layer 46 => block2b_add                   (None, 64, 64, 24)                  param = 98304 
layer 47 => block3a_expand_conv           (None, 64, 64, 144)                  param = 589824 
layer 48 => block3a_expand_bn             (None, 64, 64, 144)                  param = 589824 
layer 49 => block3a_expand_activation     (None, 64, 64, 144)                  param = 589824 
layer 50 => block3a_dwconv_pad            (None, 67, 67, 144)                  param = 646416 
layer 51 => block3a_dwconv                (None, 32, 32, 144)                  param = 147456 
layer 52 => block3a_bn                    (None, 32, 32, 144)                  param = 147456 
layer 53 => block3a_activation            (None, 32, 32, 144)                  param = 147456 
layer 54 => block3a_se_squeeze            (None, 144)                    param = 144 
layer 55 => block3a_se_reshape            (None, 1, 1, 144)                  param = 144 
layer 56 => block3a_se_reduce             (None, 1, 1, 6)                  param = 6 
layer 57 => block3a_se_expand             (None, 1, 1, 144)                  param = 144 
layer 58 => block3a_se_excite             (None, 32, 32, 144)                  param = 147456 
layer 59 => block3a_project_conv          (None, 32, 32, 40)                  param = 40960 
layer 60 => block3a_project_bn            (None, 32, 32, 40)                  param = 40960 
layer 61 => block3b_expand_conv           (None, 32, 32, 240)                  param = 245760 
layer 62 => block3b_expand_bn             (None, 32, 32, 240)                  param = 245760 
layer 63 => block3b_expand_activation     (None, 32, 32, 240)                  param = 245760 
layer 64 => block3b_dwconv                (None, 32, 32, 240)                  param = 245760 
layer 65 => block3b_bn                    (None, 32, 32, 240)                  param = 245760 
layer 66 => block3b_activation            (None, 32, 32, 240)                  param = 245760 
layer 67 => block3b_se_squeeze            (None, 240)                    param = 240 
layer 68 => block3b_se_reshape            (None, 1, 1, 240)                  param = 240 
layer 69 => block3b_se_reduce             (None, 1, 1, 10)                  param = 10 
layer 70 => block3b_se_expand             (None, 1, 1, 240)                  param = 240 
layer 71 => block3b_se_excite             (None, 32, 32, 240)                  param = 245760 
layer 72 => block3b_project_conv          (None, 32, 32, 40)                  param = 40960 
layer 73 => block3b_project_bn            (None, 32, 32, 40)                  param = 40960 
layer 74 => block3b_drop                  (None, 32, 32, 40)                  param = 40960 
layer 75 => block3b_add                   (None, 32, 32, 40)                  param = 40960 
layer 76 => block4a_expand_conv           (None, 32, 32, 240)                  param = 245760 
layer 77 => block4a_expand_bn             (None, 32, 32, 240)                  param = 245760 
layer 78 => block4a_expand_activation     (None, 32, 32, 240)                  param = 245760 
layer 79 => block4a_dwconv_pad            (None, 33, 33, 240)                  param = 261360 
layer 80 => block4a_dwconv                (None, 16, 16, 240)                  param = 61440 
layer 81 => block4a_bn                    (None, 16, 16, 240)                  param = 61440 
layer 82 => block4a_activation            (None, 16, 16, 240)                  param = 61440 
layer 83 => block4a_se_squeeze            (None, 240)                    param = 240 
layer 84 => block4a_se_reshape            (None, 1, 1, 240)                  param = 240 
layer 85 => block4a_se_reduce             (None, 1, 1, 10)                  param = 10 
layer 86 => block4a_se_expand             (None, 1, 1, 240)                  param = 240 
layer 87 => block4a_se_excite             (None, 16, 16, 240)                  param = 61440 
layer 88 => block4a_project_conv          (None, 16, 16, 80)                  param = 20480 
layer 89 => block4a_project_bn            (None, 16, 16, 80)                  param = 20480 
layer 90 => block4b_expand_conv           (None, 16, 16, 480)                  param = 122880 
layer 91 => block4b_expand_bn             (None, 16, 16, 480)                  param = 122880 
layer 92 => block4b_expand_activation     (None, 16, 16, 480)                  param = 122880 
layer 93 => block4b_dwconv                (None, 16, 16, 480)                  param = 122880 
layer 94 => block4b_bn                    (None, 16, 16, 480)                  param = 122880 
layer 95 => block4b_activation            (None, 16, 16, 480)                  param = 122880 
layer 96 => block4b_se_squeeze            (None, 480)                    param = 480 
layer 97 => block4b_se_reshape            (None, 1, 1, 480)                  param = 480 
layer 98 => block4b_se_reduce             (None, 1, 1, 20)                  param = 20 
layer 99 => block4b_se_expand             (None, 1, 1, 480)                  param = 480 
layer 100 => block4b_se_excite             (None, 16, 16, 480)                  param = 122880 
layer 101 => block4b_project_conv          (None, 16, 16, 80)                  param = 20480 
layer 102 => block4b_project_bn            (None, 16, 16, 80)                  param = 20480 
layer 103 => block4b_drop                  (None, 16, 16, 80)                  param = 20480 
layer 104 => block4b_add                   (None, 16, 16, 80)                  param = 20480 
layer 105 => block4c_expand_conv           (None, 16, 16, 480)                  param = 122880 
layer 106 => block4c_expand_bn             (None, 16, 16, 480)                  param = 122880 
layer 107 => block4c_expand_activation     (None, 16, 16, 480)                  param = 122880 
layer 108 => block4c_dwconv                (None, 16, 16, 480)                  param = 122880 
layer 109 => block4c_bn                    (None, 16, 16, 480)                  param = 122880 
layer 110 => block4c_activation            (None, 16, 16, 480)                  param = 122880 
layer 111 => block4c_se_squeeze            (None, 480)                    param = 480 
layer 112 => block4c_se_reshape            (None, 1, 1, 480)                  param = 480 
layer 113 => block4c_se_reduce             (None, 1, 1, 20)                  param = 20 
layer 114 => block4c_se_expand             (None, 1, 1, 480)                  param = 480 
layer 115 => block4c_se_excite             (None, 16, 16, 480)                  param = 122880 
layer 116 => block4c_project_conv          (None, 16, 16, 80)                  param = 20480 
layer 117 => block4c_project_bn            (None, 16, 16, 80)                  param = 20480 
layer 118 => block4c_drop                  (None, 16, 16, 80)                  param = 20480 
layer 119 => block4c_add                   (None, 16, 16, 80)                  param = 20480 
layer 120 => block5a_expand_conv           (None, 16, 16, 480)                  param = 122880 
layer 121 => block5a_expand_bn             (None, 16, 16, 480)                  param = 122880 
layer 122 => block5a_expand_activation     (None, 16, 16, 480)                  param = 122880 
layer 123 => block5a_dwconv                (None, 16, 16, 480)                  param = 122880 
layer 124 => block5a_bn                    (None, 16, 16, 480)                  param = 122880 
layer 125 => block5a_activation            (None, 16, 16, 480)                  param = 122880 
layer 126 => block5a_se_squeeze            (None, 480)                    param = 480 
layer 127 => block5a_se_reshape            (None, 1, 1, 480)                  param = 480 
layer 128 => block5a_se_reduce             (None, 1, 1, 20)                  param = 20 
layer 129 => block5a_se_expand             (None, 1, 1, 480)                  param = 480 
layer 130 => block5a_se_excite             (None, 16, 16, 480)                  param = 122880 
layer 131 => block5a_project_conv          (None, 16, 16, 112)                  param = 28672 
layer 132 => block5a_project_bn            (None, 16, 16, 112)                  param = 28672 
layer 133 => block5b_expand_conv           (None, 16, 16, 672)                  param = 172032 
layer 134 => block5b_expand_bn             (None, 16, 16, 672)                  param = 172032 
layer 135 => block5b_expand_activation     (None, 16, 16, 672)                  param = 172032 
layer 136 => block5b_dwconv                (None, 16, 16, 672)                  param = 172032 
layer 137 => block5b_bn                    (None, 16, 16, 672)                  param = 172032 
layer 138 => block5b_activation            (None, 16, 16, 672)                  param = 172032 
layer 139 => block5b_se_squeeze            (None, 672)                    param = 672 
layer 140 => block5b_se_reshape            (None, 1, 1, 672)                  param = 672 
layer 141 => block5b_se_reduce             (None, 1, 1, 28)                  param = 28 
layer 142 => block5b_se_expand             (None, 1, 1, 672)                  param = 672 
layer 143 => block5b_se_excite             (None, 16, 16, 672)                  param = 172032 
layer 144 => block5b_project_conv          (None, 16, 16, 112)                  param = 28672 
layer 145 => block5b_project_bn            (None, 16, 16, 112)                  param = 28672 
layer 146 => block5b_drop                  (None, 16, 16, 112)                  param = 28672 
layer 147 => block5b_add                   (None, 16, 16, 112)                  param = 28672 
layer 148 => block5c_expand_conv           (None, 16, 16, 672)                  param = 172032 
layer 149 => block5c_expand_bn             (None, 16, 16, 672)                  param = 172032 
layer 150 => block5c_expand_activation     (None, 16, 16, 672)                  param = 172032 
layer 151 => block5c_dwconv                (None, 16, 16, 672)                  param = 172032 
layer 152 => block5c_bn                    (None, 16, 16, 672)                  param = 172032 
layer 153 => block5c_activation            (None, 16, 16, 672)                  param = 172032 
layer 154 => block5c_se_squeeze            (None, 672)                    param = 672 
layer 155 => block5c_se_reshape            (None, 1, 1, 672)                  param = 672 
layer 156 => block5c_se_reduce             (None, 1, 1, 28)                  param = 28 
layer 157 => block5c_se_expand             (None, 1, 1, 672)                  param = 672 
layer 158 => block5c_se_excite             (None, 16, 16, 672)                  param = 172032 
layer 159 => block5c_project_conv          (None, 16, 16, 112)                  param = 28672 
layer 160 => block5c_project_bn            (None, 16, 16, 112)                  param = 28672 
layer 161 => block5c_drop                  (None, 16, 16, 112)                  param = 28672 
layer 162 => block5c_add                   (None, 16, 16, 112)                  param = 28672 
layer 163 => block6a_expand_conv           (None, 16, 16, 672)                  param = 172032 
layer 164 => block6a_expand_bn             (None, 16, 16, 672)                  param = 172032 
layer 165 => block6a_expand_activation     (None, 16, 16, 672)                  param = 172032 
layer 166 => block6a_dwconv_pad            (None, 19, 19, 672)                  param = 242592 
layer 167 => block6a_dwconv                (None, 8, 8, 672)                  param = 43008 
layer 168 => block6a_bn                    (None, 8, 8, 672)                  param = 43008 
layer 169 => block6a_activation            (None, 8, 8, 672)                  param = 43008 
layer 170 => block6a_se_squeeze            (None, 672)                    param = 672 
layer 171 => block6a_se_reshape            (None, 1, 1, 672)                  param = 672 
layer 172 => block6a_se_reduce             (None, 1, 1, 28)                  param = 28 
layer 173 => block6a_se_expand             (None, 1, 1, 672)                  param = 672 
layer 174 => block6a_se_excite             (None, 8, 8, 672)                  param = 43008 
layer 175 => block6a_project_conv          (None, 8, 8, 192)                  param = 12288 
layer 176 => block6a_project_bn            (None, 8, 8, 192)                  param = 12288 
layer 177 => block6b_expand_conv           (None, 8, 8, 1152)                  param = 73728 
layer 178 => block6b_expand_bn             (None, 8, 8, 1152)                  param = 73728 
layer 179 => block6b_expand_activation     (None, 8, 8, 1152)                  param = 73728 
layer 180 => block6b_dwconv                (None, 8, 8, 1152)                  param = 73728 
layer 181 => block6b_bn                    (None, 8, 8, 1152)                  param = 73728 
layer 182 => block6b_activation            (None, 8, 8, 1152)                  param = 73728 
layer 183 => block6b_se_squeeze            (None, 1152)                    param = 1152 
layer 184 => block6b_se_reshape            (None, 1, 1, 1152)                  param = 1152 
layer 185 => block6b_se_reduce             (None, 1, 1, 48)                  param = 48 
layer 186 => block6b_se_expand             (None, 1, 1, 1152)                  param = 1152 
layer 187 => block6b_se_excite             (None, 8, 8, 1152)                  param = 73728 
layer 188 => block6b_project_conv          (None, 8, 8, 192)                  param = 12288 
layer 189 => block6b_project_bn            (None, 8, 8, 192)                  param = 12288 
layer 190 => block6b_drop                  (None, 8, 8, 192)                  param = 12288 
layer 191 => block6b_add                   (None, 8, 8, 192)                  param = 12288 
layer 192 => block6c_expand_conv           (None, 8, 8, 1152)                  param = 73728 
layer 193 => block6c_expand_bn             (None, 8, 8, 1152)                  param = 73728 
layer 194 => block6c_expand_activation     (None, 8, 8, 1152)                  param = 73728 
layer 195 => block6c_dwconv                (None, 8, 8, 1152)                  param = 73728 
layer 196 => block6c_bn                    (None, 8, 8, 1152)                  param = 73728 
layer 197 => block6c_activation            (None, 8, 8, 1152)                  param = 73728 
layer 198 => block6c_se_squeeze            (None, 1152)                    param = 1152 
layer 199 => block6c_se_reshape            (None, 1, 1, 1152)                  param = 1152 
layer 200 => block6c_se_reduce             (None, 1, 1, 48)                  param = 48 
layer 201 => block6c_se_expand             (None, 1, 1, 1152)                  param = 1152 
layer 202 => block6c_se_excite             (None, 8, 8, 1152)                  param = 73728 
layer 203 => block6c_project_conv          (None, 8, 8, 192)                  param = 12288 
layer 204 => block6c_project_bn            (None, 8, 8, 192)                  param = 12288 
layer 205 => block6c_drop                  (None, 8, 8, 192)                  param = 12288 
layer 206 => block6c_add                   (None, 8, 8, 192)                  param = 12288 
layer 207 => block6d_expand_conv           (None, 8, 8, 1152)                  param = 73728 
layer 208 => block6d_expand_bn             (None, 8, 8, 1152)                  param = 73728 
layer 209 => block6d_expand_activation     (None, 8, 8, 1152)                  param = 73728 
layer 210 => block6d_dwconv                (None, 8, 8, 1152)                  param = 73728 
layer 211 => block6d_bn                    (None, 8, 8, 1152)                  param = 73728 
layer 212 => block6d_activation            (None, 8, 8, 1152)                  param = 73728 
layer 213 => block6d_se_squeeze            (None, 1152)                    param = 1152 
layer 214 => block6d_se_reshape            (None, 1, 1, 1152)                  param = 1152 
layer 215 => block6d_se_reduce             (None, 1, 1, 48)                  param = 48 
layer 216 => block6d_se_expand             (None, 1, 1, 1152)                  param = 1152 
layer 217 => block6d_se_excite             (None, 8, 8, 1152)                  param = 73728 
layer 218 => block6d_project_conv          (None, 8, 8, 192)                  param = 12288 
layer 219 => block6d_project_bn            (None, 8, 8, 192)                  param = 12288 
layer 220 => block6d_drop                  (None, 8, 8, 192)                  param = 12288 
layer 221 => block6d_add                   (None, 8, 8, 192)                  param = 12288 
layer 222 => block7a_expand_conv           (None, 8, 8, 1152)                  param = 73728 
layer 223 => block7a_expand_bn             (None, 8, 8, 1152)                  param = 73728 
layer 224 => block7a_expand_activation     (None, 8, 8, 1152)                  param = 73728 
layer 225 => block7a_dwconv                (None, 8, 8, 1152)                  param = 73728 
layer 226 => block7a_bn                    (None, 8, 8, 1152)                  param = 73728 
layer 227 => block7a_activation            (None, 8, 8, 1152)                  param = 73728 
layer 228 => block7a_se_squeeze            (None, 1152)                    param = 1152 
layer 229 => block7a_se_reshape            (None, 1, 1, 1152)                  param = 1152 
layer 230 => block7a_se_reduce             (None, 1, 1, 48)                  param = 48 
layer 231 => block7a_se_expand             (None, 1, 1, 1152)                  param = 1152 
layer 232 => block7a_se_excite             (None, 8, 8, 1152)                  param = 73728 
layer 233 => block7a_project_conv          (None, 8, 8, 320)                  param = 20480 
layer 234 => block7a_project_bn            (None, 8, 8, 320)                  param = 20480 
layer 235 => top_conv                      (None, 8, 8, 1280)                  param = 81920 
layer 236 => top_bn                        (None, 8, 8, 1280)                  param = 81920 
layer 237 => top_activation                (None, 8, 8, 1280)                  param = 81920 
layer 238 => avg_pool                      (None, 1280)                    param = 1280 
layer 239 => top_dropout                   (None, 1280)                    param = 1280 
layer 240 => predictions                   (None, 1000)                    param = 1000 
