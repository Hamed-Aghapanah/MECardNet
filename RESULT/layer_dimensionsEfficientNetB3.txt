layer 1 => input_tensor                  [(None, 256, 256, 3)]                     param = 1.0 
layer 2 => rescaling_13                  (None, 256, 256, 3)                  param = 196608 
layer 3 => normalization_10              (None, 256, 256, 3)                  param = 196608 
layer 4 => stem_conv_pad                 (None, 257, 257, 3)                  param = 198147 
layer 5 => stem_conv                     (None, 128, 128, 40)                  param = 655360 
layer 6 => stem_bn                       (None, 128, 128, 40)                  param = 655360 
layer 7 => stem_activation               (None, 128, 128, 40)                  param = 655360 
layer 8 => block1a_dwconv                (None, 128, 128, 40)                  param = 655360 
layer 9 => block1a_bn                    (None, 128, 128, 40)                  param = 655360 
layer 10 => block1a_activation            (None, 128, 128, 40)                  param = 655360 
layer 11 => block1a_se_squeeze            (None, 40)                    param = 40 
layer 12 => block1a_se_reshape            (None, 1, 1, 40)                  param = 40 
layer 13 => block1a_se_reduce             (None, 1, 1, 10)                  param = 10 
layer 14 => block1a_se_expand             (None, 1, 1, 40)                  param = 40 
layer 15 => block1a_se_excite             (None, 128, 128, 40)                  param = 655360 
layer 16 => block1a_project_conv          (None, 128, 128, 24)                  param = 393216 
layer 17 => block1a_project_bn            (None, 128, 128, 24)                  param = 393216 
layer 18 => block1b_dwconv                (None, 128, 128, 24)                  param = 393216 
layer 19 => block1b_bn                    (None, 128, 128, 24)                  param = 393216 
layer 20 => block1b_activation            (None, 128, 128, 24)                  param = 393216 
layer 21 => block1b_se_squeeze            (None, 24)                    param = 24 
layer 22 => block1b_se_reshape            (None, 1, 1, 24)                  param = 24 
layer 23 => block1b_se_reduce             (None, 1, 1, 6)                  param = 6 
layer 24 => block1b_se_expand             (None, 1, 1, 24)                  param = 24 
layer 25 => block1b_se_excite             (None, 128, 128, 24)                  param = 393216 
layer 26 => block1b_project_conv          (None, 128, 128, 24)                  param = 393216 
layer 27 => block1b_project_bn            (None, 128, 128, 24)                  param = 393216 
layer 28 => block1b_drop                  (None, 128, 128, 24)                  param = 393216 
layer 29 => block1b_add                   (None, 128, 128, 24)                  param = 393216 
layer 30 => block2a_expand_conv           (None, 128, 128, 144)                  param = 2359296 
layer 31 => block2a_expand_bn             (None, 128, 128, 144)                  param = 2359296 
layer 32 => block2a_expand_activation     (None, 128, 128, 144)                  param = 2359296 
layer 33 => block2a_dwconv_pad            (None, 129, 129, 144)                  param = 2396304 
layer 34 => block2a_dwconv                (None, 64, 64, 144)                  param = 589824 
layer 35 => block2a_bn                    (None, 64, 64, 144)                  param = 589824 
layer 36 => block2a_activation            (None, 64, 64, 144)                  param = 589824 
layer 37 => block2a_se_squeeze            (None, 144)                    param = 144 
layer 38 => block2a_se_reshape            (None, 1, 1, 144)                  param = 144 
layer 39 => block2a_se_reduce             (None, 1, 1, 6)                  param = 6 
layer 40 => block2a_se_expand             (None, 1, 1, 144)                  param = 144 
layer 41 => block2a_se_excite             (None, 64, 64, 144)                  param = 589824 
layer 42 => block2a_project_conv          (None, 64, 64, 32)                  param = 131072 
layer 43 => block2a_project_bn            (None, 64, 64, 32)                  param = 131072 
layer 44 => block2b_expand_conv           (None, 64, 64, 192)                  param = 786432 
layer 45 => block2b_expand_bn             (None, 64, 64, 192)                  param = 786432 
layer 46 => block2b_expand_activation     (None, 64, 64, 192)                  param = 786432 
layer 47 => block2b_dwconv                (None, 64, 64, 192)                  param = 786432 
layer 48 => block2b_bn                    (None, 64, 64, 192)                  param = 786432 
layer 49 => block2b_activation            (None, 64, 64, 192)                  param = 786432 
layer 50 => block2b_se_squeeze            (None, 192)                    param = 192 
layer 51 => block2b_se_reshape            (None, 1, 1, 192)                  param = 192 
layer 52 => block2b_se_reduce             (None, 1, 1, 8)                  param = 8 
layer 53 => block2b_se_expand             (None, 1, 1, 192)                  param = 192 
layer 54 => block2b_se_excite             (None, 64, 64, 192)                  param = 786432 
layer 55 => block2b_project_conv          (None, 64, 64, 32)                  param = 131072 
layer 56 => block2b_project_bn            (None, 64, 64, 32)                  param = 131072 
layer 57 => block2b_drop                  (None, 64, 64, 32)                  param = 131072 
layer 58 => block2b_add                   (None, 64, 64, 32)                  param = 131072 
layer 59 => block2c_expand_conv           (None, 64, 64, 192)                  param = 786432 
layer 60 => block2c_expand_bn             (None, 64, 64, 192)                  param = 786432 
layer 61 => block2c_expand_activation     (None, 64, 64, 192)                  param = 786432 
layer 62 => block2c_dwconv                (None, 64, 64, 192)                  param = 786432 
layer 63 => block2c_bn                    (None, 64, 64, 192)                  param = 786432 
layer 64 => block2c_activation            (None, 64, 64, 192)                  param = 786432 
layer 65 => block2c_se_squeeze            (None, 192)                    param = 192 
layer 66 => block2c_se_reshape            (None, 1, 1, 192)                  param = 192 
layer 67 => block2c_se_reduce             (None, 1, 1, 8)                  param = 8 
layer 68 => block2c_se_expand             (None, 1, 1, 192)                  param = 192 
layer 69 => block2c_se_excite             (None, 64, 64, 192)                  param = 786432 
layer 70 => block2c_project_conv          (None, 64, 64, 32)                  param = 131072 
layer 71 => block2c_project_bn            (None, 64, 64, 32)                  param = 131072 
layer 72 => block2c_drop                  (None, 64, 64, 32)                  param = 131072 
layer 73 => block2c_add                   (None, 64, 64, 32)                  param = 131072 
layer 74 => block3a_expand_conv           (None, 64, 64, 192)                  param = 786432 
layer 75 => block3a_expand_bn             (None, 64, 64, 192)                  param = 786432 
layer 76 => block3a_expand_activation     (None, 64, 64, 192)                  param = 786432 
layer 77 => block3a_dwconv_pad            (None, 67, 67, 192)                  param = 861888 
layer 78 => block3a_dwconv                (None, 32, 32, 192)                  param = 196608 
layer 79 => block3a_bn                    (None, 32, 32, 192)                  param = 196608 
layer 80 => block3a_activation            (None, 32, 32, 192)                  param = 196608 
layer 81 => block3a_se_squeeze            (None, 192)                    param = 192 
layer 82 => block3a_se_reshape            (None, 1, 1, 192)                  param = 192 
layer 83 => block3a_se_reduce             (None, 1, 1, 8)                  param = 8 
layer 84 => block3a_se_expand             (None, 1, 1, 192)                  param = 192 
layer 85 => block3a_se_excite             (None, 32, 32, 192)                  param = 196608 
layer 86 => block3a_project_conv          (None, 32, 32, 48)                  param = 49152 
layer 87 => block3a_project_bn            (None, 32, 32, 48)                  param = 49152 
layer 88 => block3b_expand_conv           (None, 32, 32, 288)                  param = 294912 
layer 89 => block3b_expand_bn             (None, 32, 32, 288)                  param = 294912 
layer 90 => block3b_expand_activation     (None, 32, 32, 288)                  param = 294912 
layer 91 => block3b_dwconv                (None, 32, 32, 288)                  param = 294912 
layer 92 => block3b_bn                    (None, 32, 32, 288)                  param = 294912 
layer 93 => block3b_activation            (None, 32, 32, 288)                  param = 294912 
layer 94 => block3b_se_squeeze            (None, 288)                    param = 288 
layer 95 => block3b_se_reshape            (None, 1, 1, 288)                  param = 288 
layer 96 => block3b_se_reduce             (None, 1, 1, 12)                  param = 12 
layer 97 => block3b_se_expand             (None, 1, 1, 288)                  param = 288 
layer 98 => block3b_se_excite             (None, 32, 32, 288)                  param = 294912 
layer 99 => block3b_project_conv          (None, 32, 32, 48)                  param = 49152 
layer 100 => block3b_project_bn            (None, 32, 32, 48)                  param = 49152 
layer 101 => block3b_drop                  (None, 32, 32, 48)                  param = 49152 
layer 102 => block3b_add                   (None, 32, 32, 48)                  param = 49152 
layer 103 => block3c_expand_conv           (None, 32, 32, 288)                  param = 294912 
layer 104 => block3c_expand_bn             (None, 32, 32, 288)                  param = 294912 
layer 105 => block3c_expand_activation     (None, 32, 32, 288)                  param = 294912 
layer 106 => block3c_dwconv                (None, 32, 32, 288)                  param = 294912 
layer 107 => block3c_bn                    (None, 32, 32, 288)                  param = 294912 
layer 108 => block3c_activation            (None, 32, 32, 288)                  param = 294912 
layer 109 => block3c_se_squeeze            (None, 288)                    param = 288 
layer 110 => block3c_se_reshape            (None, 1, 1, 288)                  param = 288 
layer 111 => block3c_se_reduce             (None, 1, 1, 12)                  param = 12 
layer 112 => block3c_se_expand             (None, 1, 1, 288)                  param = 288 
layer 113 => block3c_se_excite             (None, 32, 32, 288)                  param = 294912 
layer 114 => block3c_project_conv          (None, 32, 32, 48)                  param = 49152 
layer 115 => block3c_project_bn            (None, 32, 32, 48)                  param = 49152 
layer 116 => block3c_drop                  (None, 32, 32, 48)                  param = 49152 
layer 117 => block3c_add                   (None, 32, 32, 48)                  param = 49152 
layer 118 => block4a_expand_conv           (None, 32, 32, 288)                  param = 294912 
layer 119 => block4a_expand_bn             (None, 32, 32, 288)                  param = 294912 
layer 120 => block4a_expand_activation     (None, 32, 32, 288)                  param = 294912 
layer 121 => block4a_dwconv_pad            (None, 33, 33, 288)                  param = 313632 
layer 122 => block4a_dwconv                (None, 16, 16, 288)                  param = 73728 
layer 123 => block4a_bn                    (None, 16, 16, 288)                  param = 73728 
layer 124 => block4a_activation            (None, 16, 16, 288)                  param = 73728 
layer 125 => block4a_se_squeeze            (None, 288)                    param = 288 
layer 126 => block4a_se_reshape            (None, 1, 1, 288)                  param = 288 
layer 127 => block4a_se_reduce             (None, 1, 1, 12)                  param = 12 
layer 128 => block4a_se_expand             (None, 1, 1, 288)                  param = 288 
layer 129 => block4a_se_excite             (None, 16, 16, 288)                  param = 73728 
layer 130 => block4a_project_conv          (None, 16, 16, 96)                  param = 24576 
layer 131 => block4a_project_bn            (None, 16, 16, 96)                  param = 24576 
layer 132 => block4b_expand_conv           (None, 16, 16, 576)                  param = 147456 
layer 133 => block4b_expand_bn             (None, 16, 16, 576)                  param = 147456 
layer 134 => block4b_expand_activation     (None, 16, 16, 576)                  param = 147456 
layer 135 => block4b_dwconv                (None, 16, 16, 576)                  param = 147456 
layer 136 => block4b_bn                    (None, 16, 16, 576)                  param = 147456 
layer 137 => block4b_activation            (None, 16, 16, 576)                  param = 147456 
layer 138 => block4b_se_squeeze            (None, 576)                    param = 576 
layer 139 => block4b_se_reshape            (None, 1, 1, 576)                  param = 576 
layer 140 => block4b_se_reduce             (None, 1, 1, 24)                  param = 24 
layer 141 => block4b_se_expand             (None, 1, 1, 576)                  param = 576 
layer 142 => block4b_se_excite             (None, 16, 16, 576)                  param = 147456 
layer 143 => block4b_project_conv          (None, 16, 16, 96)                  param = 24576 
layer 144 => block4b_project_bn            (None, 16, 16, 96)                  param = 24576 
layer 145 => block4b_drop                  (None, 16, 16, 96)                  param = 24576 
layer 146 => block4b_add                   (None, 16, 16, 96)                  param = 24576 
layer 147 => block4c_expand_conv           (None, 16, 16, 576)                  param = 147456 
layer 148 => block4c_expand_bn             (None, 16, 16, 576)                  param = 147456 
layer 149 => block4c_expand_activation     (None, 16, 16, 576)                  param = 147456 
layer 150 => block4c_dwconv                (None, 16, 16, 576)                  param = 147456 
layer 151 => block4c_bn                    (None, 16, 16, 576)                  param = 147456 
layer 152 => block4c_activation            (None, 16, 16, 576)                  param = 147456 
layer 153 => block4c_se_squeeze            (None, 576)                    param = 576 
layer 154 => block4c_se_reshape            (None, 1, 1, 576)                  param = 576 
layer 155 => block4c_se_reduce             (None, 1, 1, 24)                  param = 24 
layer 156 => block4c_se_expand             (None, 1, 1, 576)                  param = 576 
layer 157 => block4c_se_excite             (None, 16, 16, 576)                  param = 147456 
layer 158 => block4c_project_conv          (None, 16, 16, 96)                  param = 24576 
layer 159 => block4c_project_bn            (None, 16, 16, 96)                  param = 24576 
layer 160 => block4c_drop                  (None, 16, 16, 96)                  param = 24576 
layer 161 => block4c_add                   (None, 16, 16, 96)                  param = 24576 
layer 162 => block4d_expand_conv           (None, 16, 16, 576)                  param = 147456 
layer 163 => block4d_expand_bn             (None, 16, 16, 576)                  param = 147456 
layer 164 => block4d_expand_activation     (None, 16, 16, 576)                  param = 147456 
layer 165 => block4d_dwconv                (None, 16, 16, 576)                  param = 147456 
layer 166 => block4d_bn                    (None, 16, 16, 576)                  param = 147456 
layer 167 => block4d_activation            (None, 16, 16, 576)                  param = 147456 
layer 168 => block4d_se_squeeze            (None, 576)                    param = 576 
layer 169 => block4d_se_reshape            (None, 1, 1, 576)                  param = 576 
layer 170 => block4d_se_reduce             (None, 1, 1, 24)                  param = 24 
layer 171 => block4d_se_expand             (None, 1, 1, 576)                  param = 576 
layer 172 => block4d_se_excite             (None, 16, 16, 576)                  param = 147456 
layer 173 => block4d_project_conv          (None, 16, 16, 96)                  param = 24576 
layer 174 => block4d_project_bn            (None, 16, 16, 96)                  param = 24576 
layer 175 => block4d_drop                  (None, 16, 16, 96)                  param = 24576 
layer 176 => block4d_add                   (None, 16, 16, 96)                  param = 24576 
layer 177 => block4e_expand_conv           (None, 16, 16, 576)                  param = 147456 
layer 178 => block4e_expand_bn             (None, 16, 16, 576)                  param = 147456 
layer 179 => block4e_expand_activation     (None, 16, 16, 576)                  param = 147456 
layer 180 => block4e_dwconv                (None, 16, 16, 576)                  param = 147456 
layer 181 => block4e_bn                    (None, 16, 16, 576)                  param = 147456 
layer 182 => block4e_activation            (None, 16, 16, 576)                  param = 147456 
layer 183 => block4e_se_squeeze            (None, 576)                    param = 576 
layer 184 => block4e_se_reshape            (None, 1, 1, 576)                  param = 576 
layer 185 => block4e_se_reduce             (None, 1, 1, 24)                  param = 24 
layer 186 => block4e_se_expand             (None, 1, 1, 576)                  param = 576 
layer 187 => block4e_se_excite             (None, 16, 16, 576)                  param = 147456 
layer 188 => block4e_project_conv          (None, 16, 16, 96)                  param = 24576 
layer 189 => block4e_project_bn            (None, 16, 16, 96)                  param = 24576 
layer 190 => block4e_drop                  (None, 16, 16, 96)                  param = 24576 
layer 191 => block4e_add                   (None, 16, 16, 96)                  param = 24576 
layer 192 => block5a_expand_conv           (None, 16, 16, 576)                  param = 147456 
layer 193 => block5a_expand_bn             (None, 16, 16, 576)                  param = 147456 
layer 194 => block5a_expand_activation     (None, 16, 16, 576)                  param = 147456 
layer 195 => block5a_dwconv                (None, 16, 16, 576)                  param = 147456 
layer 196 => block5a_bn                    (None, 16, 16, 576)                  param = 147456 
layer 197 => block5a_activation            (None, 16, 16, 576)                  param = 147456 
layer 198 => block5a_se_squeeze            (None, 576)                    param = 576 
layer 199 => block5a_se_reshape            (None, 1, 1, 576)                  param = 576 
layer 200 => block5a_se_reduce             (None, 1, 1, 24)                  param = 24 
layer 201 => block5a_se_expand             (None, 1, 1, 576)                  param = 576 
layer 202 => block5a_se_excite             (None, 16, 16, 576)                  param = 147456 
layer 203 => block5a_project_conv          (None, 16, 16, 136)                  param = 34816 
layer 204 => block5a_project_bn            (None, 16, 16, 136)                  param = 34816 
layer 205 => block5b_expand_conv           (None, 16, 16, 816)                  param = 208896 
layer 206 => block5b_expand_bn             (None, 16, 16, 816)                  param = 208896 
layer 207 => block5b_expand_activation     (None, 16, 16, 816)                  param = 208896 
layer 208 => block5b_dwconv                (None, 16, 16, 816)                  param = 208896 
layer 209 => block5b_bn                    (None, 16, 16, 816)                  param = 208896 
layer 210 => block5b_activation            (None, 16, 16, 816)                  param = 208896 
layer 211 => block5b_se_squeeze            (None, 816)                    param = 816 
layer 212 => block5b_se_reshape            (None, 1, 1, 816)                  param = 816 
layer 213 => block5b_se_reduce             (None, 1, 1, 34)                  param = 34 
layer 214 => block5b_se_expand             (None, 1, 1, 816)                  param = 816 
layer 215 => block5b_se_excite             (None, 16, 16, 816)                  param = 208896 
layer 216 => block5b_project_conv          (None, 16, 16, 136)                  param = 34816 
layer 217 => block5b_project_bn            (None, 16, 16, 136)                  param = 34816 
layer 218 => block5b_drop                  (None, 16, 16, 136)                  param = 34816 
layer 219 => block5b_add                   (None, 16, 16, 136)                  param = 34816 
layer 220 => block5c_expand_conv           (None, 16, 16, 816)                  param = 208896 
layer 221 => block5c_expand_bn             (None, 16, 16, 816)                  param = 208896 
layer 222 => block5c_expand_activation     (None, 16, 16, 816)                  param = 208896 
layer 223 => block5c_dwconv                (None, 16, 16, 816)                  param = 208896 
layer 224 => block5c_bn                    (None, 16, 16, 816)                  param = 208896 
layer 225 => block5c_activation            (None, 16, 16, 816)                  param = 208896 
layer 226 => block5c_se_squeeze            (None, 816)                    param = 816 
layer 227 => block5c_se_reshape            (None, 1, 1, 816)                  param = 816 
layer 228 => block5c_se_reduce             (None, 1, 1, 34)                  param = 34 
layer 229 => block5c_se_expand             (None, 1, 1, 816)                  param = 816 
layer 230 => block5c_se_excite             (None, 16, 16, 816)                  param = 208896 
layer 231 => block5c_project_conv          (None, 16, 16, 136)                  param = 34816 
layer 232 => block5c_project_bn            (None, 16, 16, 136)                  param = 34816 
layer 233 => block5c_drop                  (None, 16, 16, 136)                  param = 34816 
layer 234 => block5c_add                   (None, 16, 16, 136)                  param = 34816 
layer 235 => block5d_expand_conv           (None, 16, 16, 816)                  param = 208896 
layer 236 => block5d_expand_bn             (None, 16, 16, 816)                  param = 208896 
layer 237 => block5d_expand_activation     (None, 16, 16, 816)                  param = 208896 
layer 238 => block5d_dwconv                (None, 16, 16, 816)                  param = 208896 
layer 239 => block5d_bn                    (None, 16, 16, 816)                  param = 208896 
layer 240 => block5d_activation            (None, 16, 16, 816)                  param = 208896 
layer 241 => block5d_se_squeeze            (None, 816)                    param = 816 
layer 242 => block5d_se_reshape            (None, 1, 1, 816)                  param = 816 
layer 243 => block5d_se_reduce             (None, 1, 1, 34)                  param = 34 
layer 244 => block5d_se_expand             (None, 1, 1, 816)                  param = 816 
layer 245 => block5d_se_excite             (None, 16, 16, 816)                  param = 208896 
layer 246 => block5d_project_conv          (None, 16, 16, 136)                  param = 34816 
layer 247 => block5d_project_bn            (None, 16, 16, 136)                  param = 34816 
layer 248 => block5d_drop                  (None, 16, 16, 136)                  param = 34816 
layer 249 => block5d_add                   (None, 16, 16, 136)                  param = 34816 
layer 250 => block5e_expand_conv           (None, 16, 16, 816)                  param = 208896 
layer 251 => block5e_expand_bn             (None, 16, 16, 816)                  param = 208896 
layer 252 => block5e_expand_activation     (None, 16, 16, 816)                  param = 208896 
layer 253 => block5e_dwconv                (None, 16, 16, 816)                  param = 208896 
layer 254 => block5e_bn                    (None, 16, 16, 816)                  param = 208896 
layer 255 => block5e_activation            (None, 16, 16, 816)                  param = 208896 
layer 256 => block5e_se_squeeze            (None, 816)                    param = 816 
layer 257 => block5e_se_reshape            (None, 1, 1, 816)                  param = 816 
layer 258 => block5e_se_reduce             (None, 1, 1, 34)                  param = 34 
layer 259 => block5e_se_expand             (None, 1, 1, 816)                  param = 816 
layer 260 => block5e_se_excite             (None, 16, 16, 816)                  param = 208896 
layer 261 => block5e_project_conv          (None, 16, 16, 136)                  param = 34816 
layer 262 => block5e_project_bn            (None, 16, 16, 136)                  param = 34816 
layer 263 => block5e_drop                  (None, 16, 16, 136)                  param = 34816 
layer 264 => block5e_add                   (None, 16, 16, 136)                  param = 34816 
layer 265 => block6a_expand_conv           (None, 16, 16, 816)                  param = 208896 
layer 266 => block6a_expand_bn             (None, 16, 16, 816)                  param = 208896 
layer 267 => block6a_expand_activation     (None, 16, 16, 816)                  param = 208896 
layer 268 => block6a_dwconv_pad            (None, 19, 19, 816)                  param = 294576 
layer 269 => block6a_dwconv                (None, 8, 8, 816)                  param = 52224 
layer 270 => block6a_bn                    (None, 8, 8, 816)                  param = 52224 
layer 271 => block6a_activation            (None, 8, 8, 816)                  param = 52224 
layer 272 => block6a_se_squeeze            (None, 816)                    param = 816 
layer 273 => block6a_se_reshape            (None, 1, 1, 816)                  param = 816 
layer 274 => block6a_se_reduce             (None, 1, 1, 34)                  param = 34 
layer 275 => block6a_se_expand             (None, 1, 1, 816)                  param = 816 
layer 276 => block6a_se_excite             (None, 8, 8, 816)                  param = 52224 
layer 277 => block6a_project_conv          (None, 8, 8, 232)                  param = 14848 
layer 278 => block6a_project_bn            (None, 8, 8, 232)                  param = 14848 
layer 279 => block6b_expand_conv           (None, 8, 8, 1392)                  param = 89088 
layer 280 => block6b_expand_bn             (None, 8, 8, 1392)                  param = 89088 
layer 281 => block6b_expand_activation     (None, 8, 8, 1392)                  param = 89088 
layer 282 => block6b_dwconv                (None, 8, 8, 1392)                  param = 89088 
layer 283 => block6b_bn                    (None, 8, 8, 1392)                  param = 89088 
layer 284 => block6b_activation            (None, 8, 8, 1392)                  param = 89088 
layer 285 => block6b_se_squeeze            (None, 1392)                    param = 1392 
layer 286 => block6b_se_reshape            (None, 1, 1, 1392)                  param = 1392 
layer 287 => block6b_se_reduce             (None, 1, 1, 58)                  param = 58 
layer 288 => block6b_se_expand             (None, 1, 1, 1392)                  param = 1392 
layer 289 => block6b_se_excite             (None, 8, 8, 1392)                  param = 89088 
layer 290 => block6b_project_conv          (None, 8, 8, 232)                  param = 14848 
layer 291 => block6b_project_bn            (None, 8, 8, 232)                  param = 14848 
layer 292 => block6b_drop                  (None, 8, 8, 232)                  param = 14848 
layer 293 => block6b_add                   (None, 8, 8, 232)                  param = 14848 
layer 294 => block6c_expand_conv           (None, 8, 8, 1392)                  param = 89088 
layer 295 => block6c_expand_bn             (None, 8, 8, 1392)                  param = 89088 
layer 296 => block6c_expand_activation     (None, 8, 8, 1392)                  param = 89088 
layer 297 => block6c_dwconv                (None, 8, 8, 1392)                  param = 89088 
layer 298 => block6c_bn                    (None, 8, 8, 1392)                  param = 89088 
layer 299 => block6c_activation            (None, 8, 8, 1392)                  param = 89088 
layer 300 => block6c_se_squeeze            (None, 1392)                    param = 1392 
layer 301 => block6c_se_reshape            (None, 1, 1, 1392)                  param = 1392 
layer 302 => block6c_se_reduce             (None, 1, 1, 58)                  param = 58 
layer 303 => block6c_se_expand             (None, 1, 1, 1392)                  param = 1392 
layer 304 => block6c_se_excite             (None, 8, 8, 1392)                  param = 89088 
layer 305 => block6c_project_conv          (None, 8, 8, 232)                  param = 14848 
layer 306 => block6c_project_bn            (None, 8, 8, 232)                  param = 14848 
layer 307 => block6c_drop                  (None, 8, 8, 232)                  param = 14848 
layer 308 => block6c_add                   (None, 8, 8, 232)                  param = 14848 
layer 309 => block6d_expand_conv           (None, 8, 8, 1392)                  param = 89088 
layer 310 => block6d_expand_bn             (None, 8, 8, 1392)                  param = 89088 
layer 311 => block6d_expand_activation     (None, 8, 8, 1392)                  param = 89088 
layer 312 => block6d_dwconv                (None, 8, 8, 1392)                  param = 89088 
layer 313 => block6d_bn                    (None, 8, 8, 1392)                  param = 89088 
layer 314 => block6d_activation            (None, 8, 8, 1392)                  param = 89088 
layer 315 => block6d_se_squeeze            (None, 1392)                    param = 1392 
layer 316 => block6d_se_reshape            (None, 1, 1, 1392)                  param = 1392 
layer 317 => block6d_se_reduce             (None, 1, 1, 58)                  param = 58 
layer 318 => block6d_se_expand             (None, 1, 1, 1392)                  param = 1392 
layer 319 => block6d_se_excite             (None, 8, 8, 1392)                  param = 89088 
layer 320 => block6d_project_conv          (None, 8, 8, 232)                  param = 14848 
layer 321 => block6d_project_bn            (None, 8, 8, 232)                  param = 14848 
layer 322 => block6d_drop                  (None, 8, 8, 232)                  param = 14848 
layer 323 => block6d_add                   (None, 8, 8, 232)                  param = 14848 
layer 324 => block6e_expand_conv           (None, 8, 8, 1392)                  param = 89088 
layer 325 => block6e_expand_bn             (None, 8, 8, 1392)                  param = 89088 
layer 326 => block6e_expand_activation     (None, 8, 8, 1392)                  param = 89088 
layer 327 => block6e_dwconv                (None, 8, 8, 1392)                  param = 89088 
layer 328 => block6e_bn                    (None, 8, 8, 1392)                  param = 89088 
layer 329 => block6e_activation            (None, 8, 8, 1392)                  param = 89088 
layer 330 => block6e_se_squeeze            (None, 1392)                    param = 1392 
layer 331 => block6e_se_reshape            (None, 1, 1, 1392)                  param = 1392 
layer 332 => block6e_se_reduce             (None, 1, 1, 58)                  param = 58 
layer 333 => block6e_se_expand             (None, 1, 1, 1392)                  param = 1392 
layer 334 => block6e_se_excite             (None, 8, 8, 1392)                  param = 89088 
layer 335 => block6e_project_conv          (None, 8, 8, 232)                  param = 14848 
layer 336 => block6e_project_bn            (None, 8, 8, 232)                  param = 14848 
layer 337 => block6e_drop                  (None, 8, 8, 232)                  param = 14848 
layer 338 => block6e_add                   (None, 8, 8, 232)                  param = 14848 
layer 339 => block6f_expand_conv           (None, 8, 8, 1392)                  param = 89088 
layer 340 => block6f_expand_bn             (None, 8, 8, 1392)                  param = 89088 
layer 341 => block6f_expand_activation     (None, 8, 8, 1392)                  param = 89088 
layer 342 => block6f_dwconv                (None, 8, 8, 1392)                  param = 89088 
layer 343 => block6f_bn                    (None, 8, 8, 1392)                  param = 89088 
layer 344 => block6f_activation            (None, 8, 8, 1392)                  param = 89088 
layer 345 => block6f_se_squeeze            (None, 1392)                    param = 1392 
layer 346 => block6f_se_reshape            (None, 1, 1, 1392)                  param = 1392 
layer 347 => block6f_se_reduce             (None, 1, 1, 58)                  param = 58 
layer 348 => block6f_se_expand             (None, 1, 1, 1392)                  param = 1392 
layer 349 => block6f_se_excite             (None, 8, 8, 1392)                  param = 89088 
layer 350 => block6f_project_conv          (None, 8, 8, 232)                  param = 14848 
layer 351 => block6f_project_bn            (None, 8, 8, 232)                  param = 14848 
layer 352 => block6f_drop                  (None, 8, 8, 232)                  param = 14848 
layer 353 => block6f_add                   (None, 8, 8, 232)                  param = 14848 
layer 354 => block7a_expand_conv           (None, 8, 8, 1392)                  param = 89088 
layer 355 => block7a_expand_bn             (None, 8, 8, 1392)                  param = 89088 
layer 356 => block7a_expand_activation     (None, 8, 8, 1392)                  param = 89088 
layer 357 => block7a_dwconv                (None, 8, 8, 1392)                  param = 89088 
layer 358 => block7a_bn                    (None, 8, 8, 1392)                  param = 89088 
layer 359 => block7a_activation            (None, 8, 8, 1392)                  param = 89088 
layer 360 => block7a_se_squeeze            (None, 1392)                    param = 1392 
layer 361 => block7a_se_reshape            (None, 1, 1, 1392)                  param = 1392 
layer 362 => block7a_se_reduce             (None, 1, 1, 58)                  param = 58 
layer 363 => block7a_se_expand             (None, 1, 1, 1392)                  param = 1392 
layer 364 => block7a_se_excite             (None, 8, 8, 1392)                  param = 89088 
layer 365 => block7a_project_conv          (None, 8, 8, 384)                  param = 24576 
layer 366 => block7a_project_bn            (None, 8, 8, 384)                  param = 24576 
layer 367 => block7b_expand_conv           (None, 8, 8, 2304)                  param = 147456 
layer 368 => block7b_expand_bn             (None, 8, 8, 2304)                  param = 147456 
layer 369 => block7b_expand_activation     (None, 8, 8, 2304)                  param = 147456 
layer 370 => block7b_dwconv                (None, 8, 8, 2304)                  param = 147456 
layer 371 => block7b_bn                    (None, 8, 8, 2304)                  param = 147456 
layer 372 => block7b_activation            (None, 8, 8, 2304)                  param = 147456 
layer 373 => block7b_se_squeeze            (None, 2304)                    param = 2304 
layer 374 => block7b_se_reshape            (None, 1, 1, 2304)                  param = 2304 
layer 375 => block7b_se_reduce             (None, 1, 1, 96)                  param = 96 
layer 376 => block7b_se_expand             (None, 1, 1, 2304)                  param = 2304 
layer 377 => block7b_se_excite             (None, 8, 8, 2304)                  param = 147456 
layer 378 => block7b_project_conv          (None, 8, 8, 384)                  param = 24576 
layer 379 => block7b_project_bn            (None, 8, 8, 384)                  param = 24576 
layer 380 => block7b_drop                  (None, 8, 8, 384)                  param = 24576 
layer 381 => block7b_add                   (None, 8, 8, 384)                  param = 24576 
layer 382 => top_conv                      (None, 8, 8, 1536)                  param = 98304 
layer 383 => top_bn                        (None, 8, 8, 1536)                  param = 98304 
layer 384 => top_activation                (None, 8, 8, 1536)                  param = 98304 
layer 385 => avg_pool                      (None, 1536)                    param = 1536 
layer 386 => top_dropout                   (None, 1536)                    param = 1536 
layer 387 => predictions                   (None, 1000)                    param = 1000 
